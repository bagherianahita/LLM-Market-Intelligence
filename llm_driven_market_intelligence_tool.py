# -*- coding: utf-8 -*-
"""LLM-Driven Market Intelligence Tool

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bya-VgF_LRrRFlFc1TcQc6IiyjeKiOKT
"""

### `src/scrape.py`
```python
import requests
from bs4 import BeautifulSoup

def scrape_url(url: str) -> str:
    """Scrape text from a webpage (press release/news)."""
    resp = requests.get(url)
    soup = BeautifulSoup(resp.text, "html.parser")
    return " ".join(p.get_text() for p in soup.find_all("p"))

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--url", required=True)
    args = parser.parse_args()
    text = scrape_url(args.url)
    print(text[:500])

import os, pinecone
from openai import OpenAI
from src.scrape import scrape_url

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
pinecone.init(api_key=os.getenv("PINECONE_API_KEY"), environment=os.getenv("PINECONE_ENV"))

index = pinecone.Index(os.getenv("PINECONE_INDEX"))

def embed_text(text: str):
    """Generate embedding and store in Pinecone."""
    emb = client.embeddings.create(model="text-embedding-ada-002", input=text).data[0].embedding
    index.upsert([("doc1", emb, {"text": text})])

if __name__ == "__main__":
    sample_text = "Competitor announces new payments product."
    embed_text(sample_text)
    print("Stored in Pinecone!")

import os, pinecone
from openai import OpenAI

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
pinecone.init(api_key=os.getenv("PINECONE_API_KEY"), environment=os.getenv("PINECONE_ENV"))
index = pinecone.Index(os.getenv("PINECONE_INDEX"))

def query_rag(question: str):
    """RAG pipeline: retrieve similar docs, ask GPT-4."""
    emb = client.embeddings.create(model="text-embedding-ada-002", input=question).data[0].embedding
    results = index.query(emb, top_k=3, include_metadata=True)

    context = " ".join([r["metadata"]["text"] for r in results["matches"]])
    prompt = f"Answer the question based only on context:\n\n{context}\n\nQ: {question}\nA:"
    resp = client.chat.completions.create(model="gpt-4", messages=[{"role":"user","content":prompt}])
    return resp.choices[0].message["content"]

if __name__ == "__main__":
    print(query_rag("What are competitors doing in Q1 2025?"))

import streamlit as st
from src.query import query_rag

st.title("ðŸ“Š LLM-Driven Market Intelligence Tool")

question = st.text_input("Ask a market intelligence question:")
if question:
    with st.spinner("Fetching insights..."):
        answer = query_rag(question)
        st.success(answer)